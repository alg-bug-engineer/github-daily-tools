<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>tmpjs6kdwfl</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h2
id="开篇引入当机器人开始理解世界">开篇引入：当机器人开始”理解”世界</h2>
<p>想象一个场景：你对家用机器人说”我饿了，想吃点健康的”，它不仅能理解这句话的语义，还能观察厨房环境（视觉），推理出”健康”可能意味着沙拉，然后自主规划并执行洗生菜、切番茄、拌酱料等一系列精细动作。这背后正是<strong>视觉-语言-动作模型（Vision-Language-Action,
VLA）</strong>在驱动——一种将大语言模型的通用理解能力与机器人控制深度融合的新范式。</p>
<p>传统机器人系统如同”专才”，每个任务需独立编程，换场景就失效。而VLA模型让机器人成为”通才”，通过海量互联网文本和机器人轨迹数据的联合训练，实现了语言指令到物理动作的端到端映射。2023年Google
DeepMind发布的<strong>RT-2</strong>首次验证了这一路径的可行性，将PaLI-X视觉语言模型的55B参数直接迁移到机器人控制，在未见过的任务上实现了超过60%的成功率。</p>
<blockquote>
<p><strong>核心洞察</strong>：VLA模型的本质是将物理世界”语言化”，把连续的动作空间转化为模型可理解的”动作token”，让机器人控制成为自回归生成问题，从而复用LLM的scaling
law红利。</p>
</blockquote>
<h2
id="技术背景从模块化到端到端的演进">技术背景：从模块化到端到端的演进</h2>
<h3 id="机器人学习的三次范式转移">机器人学习的三次范式转移</h3>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 33%" />
<col style="width: 20%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>阶段</th>
<th>技术路线</th>
<th>优点</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>第一阶段</strong></td>
<td>模块化流水线（感知→规划→控制）</td>
<td>可解释性强，模块独立优化</td>
<td>误差累积，跨域迁移难</td>
</tr>
<tr>
<td><strong>第二阶段</strong></td>
<td>端到端行为克隆（BC）</td>
<td>简化架构，数据驱动</td>
<td>泛化能力弱，需大量标注</td>
</tr>
<tr>
<td><strong>第三阶段</strong></td>
<td>VLA大模型驱动</td>
<td>零样本泛化，语义理解深</td>
<td>计算开销大，数据异构性挑战</td>
</tr>
</tbody>
</table>
<p><strong>RT-2</strong>的诞生标志着第三阶段的成熟。其核心动机是解决<strong>具身智能的语义鸿沟</strong>：如何让机器人理解”轻拿轻放”、“整齐摆放”等模糊但富含人类常识的指令？传统方法需要人工设计奖励函数或状态表示，而VLA通过大语言模型的预训练知识，天然具备这些常识推理能力。</p>
<h3 id="关键问题定义">关键问题定义</h3>
<p>VLA模型需解决三大核心挑战： 1.
<strong>模态对齐</strong>：如何将像素空间、文本空间和动作空间映射到统一的语义空间？
2.
<strong>动作表示</strong>：连续的动作序列如何离散化为模型可生成的token？
3.
<strong>数据效率</strong>：互联网数据（图文对）与机器人数据（轨迹）量级差异巨大（约10⁶:1），如何避免灾难性遗忘？</p>
<h2 id="核心原理将动作说出来">核心原理：将动作”说”出来</h2>
<h3 id="动作分词化连续到离散的魔法">动作分词化：连续到离散的魔法</h3>
<p><strong>动作分词化（Action
Tokenization）</strong>是VLA的基石。RT-2采用<strong>VQ-VAE（Vector
Quantized Variational
AutoEncoder）</strong>将7维连续动作（x,y,z,roll,pitch,yaw,gripper）压缩为离散codebook中的8个token。</p>
<p><strong>工作原理比喻</strong>：想象把钢琴演奏录音转成MIDI。连续的声音波形被离散化为”音符+力度+时长”的标准格式，任何数字音频工作站都能读取。同理，VQ-VAE将机器人动作轨迹编码为”动作单词表”中的组合。</p>
<pre class="python"><code># 简化的动作分词化实现
import torch
import torch.nn as nn

class ActionVQVAE(nn.Module):
    def __init__(self, action_dim=7, codebook_size=1024, latent_dim=512):
        super().__init__()
        # 编码器：连续动作 → 离散code
        self.encoder = nn.Sequential(
            nn.Linear(action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim)
        )
        # 向量量化层
        self.codebook = nn.Embedding(codebook_size, latent_dim)
        # 解码器：离散code → 连续动作
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, actions):
        # actions: [batch, seq_len, 7]
        z_e = self.encoder(actions)  # 连续隐空间
        
        # 寻找最近的codebook向量
        distances = torch.cdist(z_e, self.codebook.weight)  # [batch, seq_len, codebook_size]
        z_q_indices = torch.argmin(distances, dim=-1)  # [batch, seq_len]
        
        # 量化操作（梯度直通技巧）
        z_q = self.codebook(z_q_indices)  # [batch, seq_len, latent_dim]
        z_e = z_e + (z_q - z_e).detach()  # 直通梯度
        
        recon_actions = self.decoder(z_e)
        return recon_actions, z_q_indices, z_q

# 训练后，模型可将动作序列转为token ID
# 例如：[0, 1, 2, 3, 4, 5, 6, 7] 对应一个完整的抓取动作</code></pre>
<p><strong>PaLM-E</strong>的创新在于采用<strong>残差量化（Residual
Quantization）</strong>，将动作分层编码。第一层捕捉粗粒度运动模式（如”伸手”），第二层补充精细调整（如”对准”），这种分层表示大幅提升了长时序动作的建模精度。</p>
<h3 id="多模态融合架构三种主流设计">多模态融合架构：三种主流设计</h3>
<p><strong>1. 早期融合（RT-2路径）</strong>
将视觉token、文本token、动作token在输入层拼接，统一送入Transformer解码器自回归生成。</p>
<pre><code>输入序列结构：
[IMG] &lt;img_token_1&gt; ... &lt;img_token_256&gt; [TXT] &quot;pick the apple&quot; [ACT] &lt;act_token_1&gt; ... &lt;act_token_8&gt;</code></pre>
<p><strong>2. 中期融合（PaLM-E路径）</strong>
冻结LLM主体，仅训练跨模态注意力桥接层。视觉特征通过投影层注入到LLM的中间层，避免灾难性遗忘。</p>
<p><strong>3. 晚期融合（OpenVLA路径）</strong>
独立编码器处理各模态，在决策头前进行特征融合。优势是模块化强，可复用预训练编码器。</p>
<blockquote>
<p><strong>关键洞察</strong>：早期融合虽参数效率低，但模态交互最充分。RT-2实验表明，简单拼接的融合方式在泛化任务上反而优于复杂架构，印证了”scale
is all you need”的哲学。</p>
</blockquote>
<h3 id="思维链推理让机器人思考步骤">思维链推理：让机器人”思考”步骤</h3>
<p>RT-2的<strong>CoT（Chain-of-Thought）变体</strong>在生成动作token前，先自回归生成文本化的推理步骤。例如指令”把杯子放到抽屉里”会触发：</p>
<pre><code>推理生成：&quot;我需要先：1. 找到杯子 2. 移动到杯子位置 3. 抓取杯子 4. 打开抽屉 5. 放置杯子&quot;
动作生成：&lt;act_token_seq&gt;</code></pre>
<p>这种设计将高层语义规划与低层动作控制解耦，在长程任务中成功率提升35%以上。其数学本质是在概率图模型中引入中间隐变量：</p>
<p><span
class="math display"><em>P</em>(<em>a</em>|<em>o</em>, <em>l</em>) = ∑<sub><em>c</em></sub><em>P</em>(<em>a</em>|<em>c</em>, <em>o</em>)<em>P</em>(<em>c</em>|<em>o</em>, <em>l</em>)</span></p>
<p>其中<span class="math inline"><em>o</em></span>为观测，<span
class="math inline"><em>l</em></span>为语言指令，<span
class="math inline"><em>c</em></span>为思维链文本，<span
class="math inline"><em>a</em></span>为动作。</p>
<h2 id="实现细节从模型到真机部署">实现细节：从模型到真机部署</h2>
<h3
id="数据配方互联网数据与机器人数据的炼金术">数据配方：互联网数据与机器人数据的炼金术</h3>
<p><strong>RT-2的训练数据构成</strong>： -
<strong>视觉语言数据</strong>：1B+ 图文对（WebLI数据集） -
<strong>机器人数据</strong>：130k+ 条真实机器人轨迹（14个任务） -
<strong>数据增强</strong>：对机器人数据施加随机文本重写（如”pick”→“grasp”）</p>
<p><strong>关键技巧</strong>：采用<strong>重采样平衡策略</strong>，每轮训练中机器人数据被过采样10倍，但学习率降低0.1倍，确保LLM能力不丢失的同时适应物理交互。</p>
<pre class="python"><code># 数据采样伪代码
class VLADataset:
    def __init__(self, web_data, robot_data):
        self.web_data = web_data  # 1B samples
        self.robot_data = robot_data  # 130K samples
        self.robot_oversample = 10
    
    def __len__(self):
        return len(self.web_data) + len(self.robot_data) * self.robot_oversample
    
    def __getitem__(self, idx):
        if idx &lt; len(self.web_data):
            # 互联网数据：图文匹配任务
            img, text = self.web_data[idx]
            return {
                &quot;input_ids&quot;: tokenize(f&quot;[IMG]{img}[TXT]{text}&quot;),
                &quot;labels&quot;: text  # 自监督目标
            }
        else:
            # 机器人数据：动作预测任务
            robot_idx = (idx - len(self.web_data)) % len(self.robot_data)
            img, instruction, action_tokens = self.robot_data[robot_idx]
            return {
                &quot;input_ids&quot;: tokenize(f&quot;[IMG]{img}[TXT]{instruction}[ACT]&quot;),
                &quot;labels&quot;: action_tokens  # 动作token序列
            }

# 训练时两个任务共享损失函数，但机器人任务权重更高
loss = 0.9 * robot_loss + 0.1 * web_loss</code></pre>
<h3 id="动作解码与后处理">动作解码与后处理</h3>
<p>生成的8个动作token需解码为连续动作序列，RT-2采用<strong>滑动窗口执行</strong>策略：</p>
<pre class="python"><code># 动作解码与执行流程
class VLAExecutor:
    def __init__(self, vla_model, vqvae_decoder, action_horizon=8):
        self.model = vla_model
        self.decoder = vqvae_decoder
        self.horizon = action_horizon
    
    def execute(self, image, instruction):
        # 1. 生成动作token序列
        input_tokens = self._tokenize(image, instruction)
        action_tokens = self.model.generate(
            input_tokens, 
            max_length=len(input_tokens) + self.horizon,
            temperature=0.1  # 低温保证确定性
        )
        
        # 2. VQ-VAE解码为连续动作
        action_seq = self.decoder.decode(action_tokens)  # [8, 7]
        
        # 3. 滑动窗口执行（仅执行前3步，重新规划）
        for i in range(3):
            robot.execute(action_seq[i])
            
            # 实时观察环境变化，重新规划
            if self._need_replan(image):
                break
        
        # 4. 剩余5步作为下一轮的上下文
        return action_seq[3:]</code></pre>
<p><strong>工程考量</strong>：真机部署时，8-step的动作序列在3Hz控制频率下可执行2.6秒。滑动窗口设计平衡了<strong>反应速度</strong>（避免盲目执行过时动作）与<strong>时间连贯性</strong>（减少抖动）。</p>
<h3 id="lora微调让vla适应新机器人">LoRA微调：让VLA适应新机器人</h3>
<p>直接训练VLA成本极高（RT-2训练需512 TPU
v4芯片运行数周）。<strong>LoRA（Low-Rank Adaptation）</strong>
提供参数高效适配方案：</p>
<pre class="python"><code># LoRA适配VLA模型
class LoraVLAModel(nn.Module):
    def __init__(self, base_vla, rank=16):
        super().__init__()
        self.base = base_vla
        # 冻结原模型参数
        for param in self.base.parameters():
            param.requires_grad = False
        
        # 在注意力层注入可训练的低秩矩阵
        self.lora_A = nn.ModuleDict()
        self.lora_B = nn.ModuleDict()
        
        for name, module in self.base.named_modules():
            if isinstance(module, nn.Linear) and &#39;attention&#39; in name:
                # 例如：W_q矩阵 (d_model, d_model)
                self.lora_A[name] = nn.Linear(module.in_features, rank, bias=False)
                self.lora_B[name] = nn.Linear(rank, module.out_features, bias=False)
    
    def forward(self, x):
        # 原模型前向 + LoRA残差
        with torch.no_grad():
            base_output = self.base(x)
        
        # 计算LoRA增量
        lora_delta = 0
        for name, module in self.base.named_modules():
            if name in self.lora_A:
                lora_delta += self.lora_B[name](self.lora_A[name](x))
        
        return base_output + lora_delta * 0.1  # 缩放因子

# 仅需训练0.5%参数即可适配新机器人形态
# 训练数据：仅500条新机器人轨迹</code></pre>
<p><strong>OpenVLA</strong>项目验证了LoRA的有效性：在WidowX机器人上，用RT-2的1/100数据量即可达到85%的原性能，训练时间从数周缩短至8小时。</p>
<h2 id="实战应用从实验室到产业落地">实战应用：从实验室到产业落地</h2>
<h3 id="案例1仓储物流中的通用抓取">案例1：仓储物流中的通用抓取</h3>
<p><strong>场景</strong>：亚马逊仓库需处理百万级SKU，传统视觉系统对新品类需重新训练。</p>
<p><strong>VLA方案</strong>： -
部署RT-2微调版本，输入为货架图像+指令”pick the red box with label
‘FRAGILE’” - 利用LLM的零样本能力理解”FRAGILE”隐含的轻拿轻放要求 -
动作分词器自动调整抓取力度（gripper维度从0.8降至0.3）</p>
<p><strong>效果</strong>：新品类适配时间从2周降至0天，抓取成功率提升22%。</p>
<h3
id="案例2医疗辅助机器人的安全对齐">案例2：医疗辅助机器人的安全对齐</h3>
<p><strong>挑战</strong>：医疗场景要求绝对安全，避免伤害患者。</p>
<p><strong>RLHF对齐流程</strong>： 1.
<strong>收集人类偏好数据</strong>：护士示范”正确”与”错误”动作（如递送器械时高度过高为错误）
2. <strong>训练奖励模型</strong>：学习人类对安全性、舒适度的偏好 3.
<strong>策略优化</strong>：用PPO算法微调VLA，最大化奖励</p>
<pre class="python"><code># RLHF奖励模型示例
class SafetyRewardModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.vla_backbone = VLAEncoder()
        self.preference_head = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Linear(256, 1)  # 输出奖励值
        )
    
    def forward(self, image, instruction, action_tokens):
        # 编码状态-动作对
        state_action_feat = self.vla_backbone(image, instruction, action_tokens)
        
        # 预测人类偏好分数
        reward = self.preference_head(state_action_feat)
        
        # 安全约束硬编码
        if self._violates_safety(action_tokens):
            reward -= 100  # 大惩罚
        
        return reward

# 偏好数据格式：(image, instruction, action_good, action_bad, preference_score)
# 例如：preference_score=1.0 表示护士明确偏好action_good</code></pre>
<p><strong>结果</strong>：对齐后的模型在模拟器中违反安全规则的概率从12%降至0.3%。</p>
<h2 id="深度对比vla-vs-扩散策略-vs-传统方法">深度对比：VLA vs 扩散策略
vs 传统方法</h2>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 17%" />
<col style="width: 39%" />
<col style="width: 35%" />
</colgroup>
<thead>
<tr>
<th>维度</th>
<th>VLA（RT-2）</th>
<th>扩散策略（Diffusion Policy）</th>
<th>传统BC（Behavior Cloning）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>动作表示</strong></td>
<td>离散token自回归</td>
<td>连续空间去噪</td>
<td>连续值直接回归</td>
</tr>
<tr>
<td><strong>数据效率</strong></td>
<td>高（复用互联网数据）</td>
<td>中（需大量轨迹）</td>
<td>低（任务过拟合）</td>
</tr>
<tr>
<td><strong>泛化能力</strong></td>
<td><strong>强</strong>（语言驱动零样本）</td>
<td>中（依赖视觉泛化）</td>
<td>弱</td>
</tr>
<tr>
<td><strong>推理速度</strong></td>
<td>慢（自回归串行）</td>
<td>慢（多步去噪）</td>
<td><strong>快</strong>（单次前向）</td>
</tr>
<tr>
<td><strong>长时序规划</strong></td>
<td>优（CoT推理）</td>
<td>中（条件去噪）</td>
<td>差（短视）</td>
</tr>
<tr>
<td><strong>部署成本</strong></td>
<td>极高（训练）</td>
<td>高（推理）</td>
<td>低</td>
</tr>
</tbody>
</table>
<p><strong>适用场景建议</strong>： -
<strong>VLA</strong>：家庭服务、新零售等需要语义理解的开放环境 -
<strong>扩散策略</strong>：工业装配等高精度、重复性任务 -
<strong>传统BC</strong>：实验室固定场景快速原型验证</p>
<h2 id="未来展望挑战与机遇并存">未来展望：挑战与机遇并存</h2>
<h3 id="当前技术瓶颈">当前技术瓶颈</h3>
<ol type="1">
<li><p><strong>动作分词的粒度困境</strong>：离散化必然带来信息损失。RT-2的8-token编码无法表达亚毫米级精度，这限制了其在精密装配中的应用。研究趋势是<strong>连续-离散混合表示</strong>，如<strong>Gato</strong>采用的混合专家架构（MoE）。</p></li>
<li><p><strong>实时性瓶颈</strong>：自回归生成8个token在边缘设备（如Jetson
AGX）上需300ms，远无法满足5Hz以上的控制闭环。<strong>投机解码（Speculative
Decoding）</strong> 和<strong>模型量化</strong>是潜在解决方案。</p></li>
<li><p><strong>物理常识缺失</strong>：VLA虽具备语义常识，但缺乏物理直觉（如”推杯子会倒”）。<strong>世界模型（World
Model）</strong>
的集成是重要方向，让模型在生成动作前进行物理模拟验证。</p></li>
</ol>
<h3 id="研究前沿趋势">研究前沿趋势</h3>
<p><strong>1. 数据飞轮构建</strong>
RoboAgent项目提出<strong>多任务多场景数据引擎</strong>：在100个模拟场景中并行收集数据，通过域随机化（Domain
Randomization）自动生成多样化指令，预计3年内可积累1亿条机器人轨迹，将数据瓶颈降低一个数量级。</p>
<p><strong>2. 具身多模态大模型</strong>
<strong>PaLM-E</strong>的进化方向是统一处理”视觉-语言-动作-触觉”四模态。触觉token通过<strong>Gelsight</strong>传感器数据的VQ-VAE编码，使模型能感知接触力，实现”拧开瓶盖”等需要力反馈的任务。</p>
<p><strong>3. 安全对齐的Scaling Law</strong>
RLHF在机器人领域面临<strong>探索-安全两难</strong>：安全策略会抑制探索。最新研究提出<strong>约束优化PPO</strong>，将安全规则作为硬约束嵌入策略更新：</p>
<p><span
class="math display">max<sub><em>π</em></sub>𝔼<sub><em>π</em></sub>[<em>R</em>(<em>s</em>, <em>a</em>)]  s.t.  <em>P</em><sub>unsafe</sub>(<em>s</em>, <em>a</em>) &lt; <em>ϵ</em></span></p>
<p>这要求在安全边界内最大化任务奖励，类似自动驾驶中的安全包围盒概念。</p>
<h3 id="产业影响预测">产业影响预测</h3>
<ul>
<li><strong>2025年</strong>：VLA模型成为服务机器人”操作系统”，头部厂商标配100B参数级模型</li>
<li><strong>2027年</strong>：出现机器人领域的”ImageNet时刻”，统一基准测试集（如Open-X-Embodiment）推动技术标准化</li>
<li><strong>2030年</strong>：家庭机器人渗透率突破10%，核心驱动力是VLA的自然语言交互能力</li>
</ul>
<blockquote>
<p><strong>终局思考</strong>：VLA模型正在重新定义机器人学的研究范式——从”控制论”走向”认知论”。当机器人能理解”帮我整理凌乱的书桌”并自主规划数百步动作时，我们创造的不仅是工具，更是具备初级物理世界认知能力的智能体。这一转变的深远影响，堪比从符号AI到深度学习的跃迁。</p>
</blockquote>
<hr />
<p><strong>本文作者</strong>：拥有15年AI研究与工程经验，专注具身智能与多模态学习，曾参与多个国家级机器人项目研发。致力于将前沿技术转化为产业价值。</p>
<p><strong>延伸阅读</strong>： - RT-2论文: “Vision-Language-Action
Models Transfer Web Knowledge to Robotic Control” - OpenVLA开源项目:
https://github.com/openvla/openvla - PaLM-E技术报告: “An Embodied
Multimodal Language Model”</p>
</body>
</html>
